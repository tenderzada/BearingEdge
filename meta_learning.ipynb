{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131f63ba-07e4-403f-aea5-c5b6d803ed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ic-lab/miniconda3/envs/torch_v1.9_gpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d683026-5fa7-4d51-aa01-6aa6d6bb7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_datasets = []\n",
    "\n",
    "# Task 1\n",
    "task1_data = np.load('./cwru1_X_train.npy')\n",
    "task1_labels = np.load('./cwru2_y_train.npy')\n",
    "\n",
    "# Task 2\n",
    "task2_data = np.load('./cwru2_X_train.npy')\n",
    "task2_labels = np.load('./cwru2_y_train.npy')\n",
    "\n",
    "# Task 3\n",
    "task3_data = np.load('./cwru3_X_train.npy')\n",
    "task3_labels = np.load('./cwru3_y_train.npy')\n",
    "\n",
    "# Task 4\n",
    "task4_data = np.load('./cwru4_X_train.npy')\n",
    "task4_labels = np.load('./cwru4_y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd1cf21-190a-4e5f-b3d9-bdc46e4dba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "# Task 1\n",
    "task1_data = torch.from_numpy(task1_data).transpose(1, 2)\n",
    "task1_labels = torch.from_numpy(task1_labels)\n",
    "\n",
    "# Task 2\n",
    "task2_data = torch.from_numpy(task2_data).transpose(1, 2)\n",
    "task2_labels = torch.from_numpy(task2_labels)\n",
    "\n",
    "# Task 3\n",
    "task3_data = torch.from_numpy(task3_data).transpose(1, 2)\n",
    "task3_labels = torch.from_numpy(task3_labels)\n",
    "\n",
    "# Task 4\n",
    "task4_data = torch.from_numpy(task4_data).transpose(1, 2)\n",
    "task4_labels = torch.from_numpy(task4_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017a9c72-7c32-4c59-b07a-a1f63a8462c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for task 1~4\n",
    "dataset1 = torch.utils.data.TensorDataset(task1_data, task1_labels)\n",
    "dataset2 = torch.utils.data.TensorDataset(task2_data, task2_labels)\n",
    "dataset3 = torch.utils.data.TensorDataset(task3_data, task3_labels)\n",
    "dataset4 = torch.utils.data.TensorDataset(task4_data, task4_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e3158f-18ee-4d64-a0ff-4b95d12acc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train_size = int(0.8 * len(dataset1))\n",
    "test_size = len(dataset1) - train_size\n",
    "\n",
    "train_dataset1, test_dataset1 = random_split(dataset1, [train_size, test_size])\n",
    "train_dataset2, test_dataset2 = random_split(dataset2, [train_size, test_size])\n",
    "train_dataset3, test_dataset3 = random_split(dataset3, [train_size, test_size])\n",
    "train_dataset4, test_dataset4 = random_split(dataset4, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eac2aea-bee5-4d75-88bb-20a2ac502f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_datasets.append((train_dataset1, test_dataset1))\n",
    "task_datasets.append((train_dataset2, test_dataset2))\n",
    "task_datasets.append((train_dataset3, test_dataset3))\n",
    "task_datasets.append((train_dataset4, test_dataset4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa77e363-5919-4222-9f7d-f329f5ea740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义WDCNN模型\n",
    "class WDCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(WDCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836f8fdc-4279-459d-9475-a7123c479dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Define the meta-learning algorithm (MAML)\n",
    "def maml_bearing_fault_diagnosis(tasks, num_epochs_inner=5, num_epochs_outer=10, batch_size=8, lr_inner=0.01, lr_outer=0.001):\n",
    "    # Initialize the WDCNN backbone\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = WDCNN(num_classes).to(device)\n",
    "    model.double()\n",
    "    model.train()\n",
    "    \n",
    "    # Define the optimizer for inner-loop adaptation\n",
    "    optimizer_inner = optim.SGD(model.parameters(), lr=lr_inner)\n",
    "    \n",
    "    # Define the optimizer for outer-loop updates\n",
    "    optimizer_outer = optim.Adam(model.parameters(), lr=lr_outer)\n",
    "    \n",
    "    # Define the optimizer for the meta-updates step\n",
    "    meta_optimizer = optim.Adam(model.parameters(), lr=lr_outer)\n",
    "    \n",
    "    # Meta-training loop\n",
    "    for epoch_outer in range(num_epochs_outer):\n",
    "        # Accumulate the meta-gradients\n",
    "        meta_gradients = [torch.zeros_like(param) for param in model.parameters()]\n",
    "        \n",
    "        for task in tasks:\n",
    "            # Split the data into support set and query set\n",
    "            support_set, query_set = task[1], task[0]\n",
    "            \n",
    "            # Inner-loop adaptation\n",
    "            for epoch_inner in range(num_epochs_inner):\n",
    "                for inputs, labels in DataLoader(support_set, batch_size=batch_size, shuffle=True):\n",
    "                    inputs = inputs.to(device, dtype=torch.double)\n",
    "                    labels = labels.to(device, dtype=torch.long)\n",
    "                    \n",
    "                    optimizer_inner.zero_grad()\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_inner.step()\n",
    "            \n",
    "            # Compute the meta-gradients\n",
    "            for inputs, labels in DataLoader(query_set, batch_size=batch_size, shuffle=True):\n",
    "                inputs = inputs.to(device, dtype=torch.double)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                \n",
    "                # MAML\n",
    "                # gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "                # meta_gradients = [meta_gradients[i] + gradients[i] for i in range(len(gradients))]\n",
    "                \n",
    "                # FOMAML\n",
    "                # Perform the meta-update step\n",
    "                meta_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                meta_optimizer.step()\n",
    "                \n",
    "        # MAML\n",
    "        # Update the model parameters with meta-gradients\n",
    "        # for param, meta_gradient in zip(model.parameters(), meta_gradients):\n",
    "        #     param.grad = meta_gradient / len(tasks)\n",
    "        # optimizer_outer.step()\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ea89ef-71d6-4911-8b0e-7addffcac393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks (training and testing sets)\n",
    "# tasks = task_datasets\n",
    "# Train the meta-learning model\n",
    "meta_model = maml_bearing_fault_diagnosis(task_datasets)\n",
    "# save model\n",
    "torch.save(meta_model.state_dict(), './parameters/meta_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e5b0d80-4004-4d42-9e13-e8c44eed1b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WDCNN(\n",
       "  (conv1): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WDCNN(num_classes).to(device)\n",
    "model.load_state_dict(torch.load('./parameters/meta_model.pt'))\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a94ae64a-b3c8-47de-b9d6-ff64a32f9b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Accuracy: 82.54%\n",
      "Task 1 Loss: 1.0807\n",
      "Task 2 Accuracy: 80.64%\n",
      "Task 2 Loss: 1.0018\n",
      "Task 3 Accuracy: 88.89%\n",
      "Task 3 Loss: 0.4365\n",
      "Task 4 Accuracy: 99.96%\n",
      "Task 4 Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Evaluate the model on testing sets of all tasks\n",
    "for i, task in enumerate(task_datasets):\n",
    "    # test_set = task[1]\n",
    "    test_loader = torch.utils.data.DataLoader(task[1], batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    task_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device, dtype=torch.double)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # inputs = inputs.to(device)  # 移动到设备上\n",
    "            # labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            task_loss += loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    loss_avg = task_loss / len(test_loader)\n",
    "    # task_accuracies.append(accuracy)\n",
    "    # task_losses.append(loss_avg)\n",
    "\n",
    "    print(f\"Task {i+1} Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Task {i+1} Loss: {loss_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2ef57-9480-45a4-86f1-a57cbb836de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_v1.9_gpu",
   "language": "python",
   "name": "torch_v1.9_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
